{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import numpy as np\n",
    "import warnings\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sirius/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_level(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    print(root)\n",
    "    sent_id = []\n",
    "    sent_text = []\n",
    "    opinion_target = []\n",
    "    opinion_category = []\n",
    "    opinion_polarity = []\n",
    "    for review in root.findall('Review'):\n",
    "        for sent in review.findall('./sentences/sentence'):\n",
    "            sent_id.append(sent.get('id'))\n",
    "            sent_text.append(sent.find('text').text)\n",
    "            target = \"\"\n",
    "            polarity = \"\"\n",
    "            category = \"\"\n",
    "            for opinion in sent.findall('./Opinions/Opinion'):\n",
    "                target += \" \" + opinion.get('target')\n",
    "                polarity += \" \" + opinion.get('polarity')\n",
    "                category += \" \" + opinion.get('category')\n",
    "            opinion_target.append(target)\n",
    "            opinion_category.append(category)\n",
    "            opinion_polarity.append(polarity)\n",
    "    return sent_id, sent_text, opinion_target, opinion_category, opinion_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml parser\n",
    "def get_list(path):\n",
    "    tree=ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    text_list = []\n",
    "    opinion_list = []\n",
    "    for review in root.findall('Review'):\n",
    "        text_string=\"\"\n",
    "        opinion_inner_list=[]\n",
    "        for sent in review.findall('./sentences/sentence'):\n",
    "            text_string= text_string+ \" \"+ sent.find('text').text\n",
    "        text_list.append(text_string)\n",
    "        for opinion in review.findall('./Opinions/Opinion'):\n",
    "            opinion_dict = {\n",
    "                opinion.get('category').replace('#','_'): opinion.get('polarity')\n",
    "            }\n",
    "            opinion_inner_list.append(opinion_dict)\n",
    "        opinion_list.append(opinion_inner_list)\n",
    "    return text_list,opinion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_level2(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    print(root)\n",
    "    sent_id = []\n",
    "    sent_text = []\n",
    "    opinion_target = []\n",
    "    opinion_category = []\n",
    "    opinion_polarity = []\n",
    "    for review in root.findall('Review'):\n",
    "        for sent in review.findall('./sentences/sentence'):\n",
    "            try:\n",
    "                sid = sent.get('id')\n",
    "                text = sent.find('text').text\n",
    "                target = \"\"\n",
    "                polarity = \"\"\n",
    "                category = \"\"\n",
    "                num_opinion_units = len(sent.findall('./Opinions/Opinion'))\n",
    "                if num_opinion_units>1:\n",
    "                    for i,opinion in enumerate(sent.findall('./Opinions/Opinion')):\n",
    "                        target = opinion.get('target')\n",
    "                        polarity = opinion.get('polarity')\n",
    "                        category = opinion.get('category')\n",
    "                        splitter = None\n",
    "                        if len(text.split(\"and\")) == num_opinion_units:\n",
    "                            splitter = 'and'\n",
    "                        elif len(text.split(\"but\")) == num_opinion_units:\n",
    "                            splitter = \"but\"\n",
    "                        elif len(text.split(\",\")) == num_opinion_units:\n",
    "                            splitter = \",\"\n",
    "                        if splitter:\n",
    "                            text1 = text.split(splitter)[i]\n",
    "                        else:\n",
    "                            text1 = text\n",
    "                        id1 = sid + chr(97+i)\n",
    "                        sent_id.append(id1)\n",
    "                        sent_text.append(text1)\n",
    "                        opinion_target.append(target)\n",
    "                        opinion_category.append(category)\n",
    "                        opinion_polarity.append(polarity)\n",
    "                else:\n",
    "                    for opinion in sent.findall('./Opinions/Opinion'):\n",
    "                        target = opinion.get('target')\n",
    "                        polarity = opinion.get('polarity')\n",
    "                        category = opinion.get('category')\n",
    "                    sent_id.append(sid)\n",
    "                    sent_text.append(text)\n",
    "                    opinion_target.append(target)\n",
    "                    opinion_category.append(category)\n",
    "                    opinion_polarity.append(polarity)\n",
    "            except:\n",
    "                print(sent.get('id'))\n",
    "    return sent_id, sent_text, opinion_target, opinion_category, opinion_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'Reviews' at 0x7f858093e4d0>\n"
     ]
    }
   ],
   "source": [
    "train_sent_id, train_text,train_opinion_target,train_opinion_category, train_opinion_polarity  = parse_sentence_level2(\"ABSA16_Restaurants_Train_SB1_v2.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = pd.DataFrame([train_sent_id, train_text,train_opinion_target,train_opinion_category, train_opinion_polarity],\n",
    "            index=['sentence_id','text','aspect_target','aspect_category','polarity']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>aspect_target</th>\n",
       "      <th>aspect_category</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1004293:0</td>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>place</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1004293:1</td>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>staff</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1004293:2</td>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1004293:3a</td>\n",
       "      <td>The food was lousy - too sweet or too salty</td>\n",
       "      <td>food</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1004293:3b</td>\n",
       "      <td>the portions tiny.</td>\n",
       "      <td>portions</td>\n",
       "      <td>FOOD#STYLE_OPTIONS</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2794</td>\n",
       "      <td>FF#10:8</td>\n",
       "      <td>The waitress came to check in on us every few ...</td>\n",
       "      <td>waitress</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2795</td>\n",
       "      <td>FF#10:9</td>\n",
       "      <td>I couldn't ignore the fact that she reach over...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>FF#10:10</td>\n",
       "      <td>She then put the check down without asking if ...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2797</td>\n",
       "      <td>FF#10:11a</td>\n",
       "      <td>I wish I could like this place more,</td>\n",
       "      <td>place</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2798</td>\n",
       "      <td>FF#10:11b</td>\n",
       "      <td>I wish someone would retrain the staff.</td>\n",
       "      <td>staff</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2799 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id                                               text  \\\n",
       "0      1004293:0  Judging from previous posts this used to be a ...   \n",
       "1      1004293:1  We, there were four of us, arrived at noon - t...   \n",
       "2      1004293:2  They never brought us complimentary noodles, i...   \n",
       "3     1004293:3a       The food was lousy - too sweet or too salty    \n",
       "4     1004293:3b                                 the portions tiny.   \n",
       "...          ...                                                ...   \n",
       "2794     FF#10:8  The waitress came to check in on us every few ...   \n",
       "2795     FF#10:9  I couldn't ignore the fact that she reach over...   \n",
       "2796    FF#10:10  She then put the check down without asking if ...   \n",
       "2797   FF#10:11a              I wish I could like this place more,    \n",
       "2798   FF#10:11b            I wish someone would retrain the staff.   \n",
       "\n",
       "     aspect_target     aspect_category  polarity  \n",
       "0            place  RESTAURANT#GENERAL  negative  \n",
       "1            staff     SERVICE#GENERAL  negative  \n",
       "2             NULL     SERVICE#GENERAL  negative  \n",
       "3             food        FOOD#QUALITY  negative  \n",
       "4         portions  FOOD#STYLE_OPTIONS  negative  \n",
       "...            ...                 ...       ...  \n",
       "2794      waitress     SERVICE#GENERAL  negative  \n",
       "2795          NULL     SERVICE#GENERAL  negative  \n",
       "2796          NULL     SERVICE#GENERAL  negative  \n",
       "2797         place  RESTAURANT#GENERAL  negative  \n",
       "2798         staff     SERVICE#GENERAL  negative  \n",
       "\n",
       "[2799 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_category\n",
      "FOOD#QUALITY                849\n",
      "SERVICE#GENERAL             449\n",
      "RESTAURANT#GENERAL          422\n",
      "                            292\n",
      "AMBIENCE#GENERAL            255\n",
      "FOOD#STYLE_OPTIONS          137\n",
      "RESTAURANT#MISCELLANEOUS     98\n",
      "FOOD#PRICES                  90\n",
      "RESTAURANT#PRICES            80\n",
      "DRINKS#QUALITY               47\n",
      "DRINKS#STYLE_OPTIONS         32\n",
      "LOCATION#GENERAL             28\n",
      "DRINKS#PRICES                20\n",
      "dtype: int64\n",
      "number of categories 13\n"
     ]
    }
   ],
   "source": [
    "print(reviews_train.groupby('aspect_category').size().sort_values(ascending=False))\n",
    "\n",
    "#how many categories\n",
    "print(\"number of categories\",reviews_train.aspect_category.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2799, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1657\n",
       "negative     749\n",
       "             292\n",
       "neutral      101\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train.to_excel(\"20200204_Cleaned_df_v2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "absa_model = Sequential()\n",
    "absa_model.add(Dense(512, input_shape=(6000,), activation='relu'))\n",
    "absa_model.add((Dense(256, activation='relu')))\n",
    "absa_model.add((Dense(128, activation='relu')))\n",
    "absa_model.add(Dense(13, activation='softmax'))\n",
    "#compile model\n",
    "absa_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6000 # We set a maximum size for the vocabulary\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(reviews_train.text)\n",
    "reviews_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(reviews_train.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_category = label_encoder.fit_transform(reviews_train.aspect_category)\n",
    "dummy_category = to_categorical(integer_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'neutral', ''], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture\n",
    "sentiment_model = Sequential()\n",
    "sentiment_model.add(Dense(512, input_shape=(6000,), activation='relu'))\n",
    "sentiment_model.add((Dense(256, activation='relu')))\n",
    "sentiment_model.add((Dense(128, activation='relu')))\n",
    "sentiment_model.add(Dense(4, activation='softmax'))\n",
    "#compile model\n",
    "sentiment_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "#create a word embedding of reviews data\n",
    "vocab_size = 6000 # We set a maximum size for the vocabulary\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(reviews_train.text)\n",
    "reviews_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(reviews_train.text))\n",
    "\n",
    "#encode the label variable\n",
    "label_encoder2 = LabelEncoder()\n",
    "integer_sentiment = label_encoder2.fit_transform(reviews_train.polarity)\n",
    "dummy_sentiment = to_categorical(integer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train.polarity.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2799/2799 [==============================] - 1s 299us/step - loss: 0.2156 - accuracy: 0.8803\n",
      "Epoch 2/100\n",
      "2799/2799 [==============================] - 1s 296us/step - loss: 0.2075 - accuracy: 0.8821\n",
      "Epoch 3/100\n",
      "2799/2799 [==============================] - 1s 299us/step - loss: 0.2047 - accuracy: 0.8857\n",
      "Epoch 4/100\n",
      "2799/2799 [==============================] - 1s 299us/step - loss: 0.2023 - accuracy: 0.8839\n",
      "Epoch 5/100\n",
      "2799/2799 [==============================] - 1s 297us/step - loss: 0.2015 - accuracy: 0.8860\n",
      "Epoch 6/100\n",
      "2799/2799 [==============================] - 1s 295us/step - loss: 0.2034 - accuracy: 0.8792\n",
      "Epoch 7/100\n",
      "2799/2799 [==============================] - 1s 297us/step - loss: 0.1988 - accuracy: 0.8807\n",
      "Epoch 8/100\n",
      "2799/2799 [==============================] - 1s 301us/step - loss: 0.1993 - accuracy: 0.8878\n",
      "Epoch 9/100\n",
      "2799/2799 [==============================] - 1s 300us/step - loss: 0.1939 - accuracy: 0.8900\n",
      "Epoch 10/100\n",
      "2799/2799 [==============================] - 1s 286us/step - loss: 0.1947 - accuracy: 0.8828\n",
      "Epoch 11/100\n",
      "2799/2799 [==============================] - 1s 303us/step - loss: 0.1920 - accuracy: 0.8857\n",
      "Epoch 12/100\n",
      "2799/2799 [==============================] - 1s 287us/step - loss: 0.1919 - accuracy: 0.8857\n",
      "Epoch 13/100\n",
      "2799/2799 [==============================] - 1s 287us/step - loss: 0.1912 - accuracy: 0.8846\n",
      "Epoch 14/100\n",
      "2799/2799 [==============================] - 1s 286us/step - loss: 0.1882 - accuracy: 0.8828\n",
      "Epoch 15/100\n",
      "2799/2799 [==============================] - 1s 283us/step - loss: 0.1883 - accuracy: 0.8871\n",
      "Epoch 16/100\n",
      "2799/2799 [==============================] - 1s 285us/step - loss: 0.1878 - accuracy: 0.8864\n",
      "Epoch 17/100\n",
      "2799/2799 [==============================] - 1s 282us/step - loss: 0.1905 - accuracy: 0.8825\n",
      "Epoch 18/100\n",
      "2799/2799 [==============================] - 1s 281us/step - loss: 0.1882 - accuracy: 0.8864\n",
      "Epoch 19/100\n",
      "2799/2799 [==============================] - 1s 285us/step - loss: 0.1866 - accuracy: 0.8867\n",
      "Epoch 20/100\n",
      "2799/2799 [==============================] - 1s 284us/step - loss: 0.1862 - accuracy: 0.8896\n",
      "Epoch 21/100\n",
      "2799/2799 [==============================] - 1s 289us/step - loss: 0.1863 - accuracy: 0.8878\n",
      "Epoch 22/100\n",
      "2799/2799 [==============================] - 1s 285us/step - loss: 0.1849 - accuracy: 0.88820s - los\n",
      "Epoch 23/100\n",
      "2799/2799 [==============================] - 1s 284us/step - loss: 0.1855 - accuracy: 0.8853\n",
      "Epoch 24/100\n",
      "2799/2799 [==============================] - 1s 291us/step - loss: 0.1835 - accuracy: 0.8907\n",
      "Epoch 25/100\n",
      "2799/2799 [==============================] - 1s 281us/step - loss: 0.1850 - accuracy: 0.8846\n",
      "Epoch 26/100\n",
      "2799/2799 [==============================] - 1s 280us/step - loss: 0.1846 - accuracy: 0.8889\n",
      "Epoch 27/100\n",
      "2799/2799 [==============================] - 1s 280us/step - loss: 0.1840 - accuracy: 0.8860\n",
      "Epoch 28/100\n",
      "2799/2799 [==============================] - 1s 312us/step - loss: 0.1862 - accuracy: 0.8853\n",
      "Epoch 29/100\n",
      "2799/2799 [==============================] - 1s 283us/step - loss: 0.1859 - accuracy: 0.8817\n",
      "Epoch 30/100\n",
      "2799/2799 [==============================] - 1s 284us/step - loss: 0.1825 - accuracy: 0.8889\n",
      "Epoch 31/100\n",
      "2799/2799 [==============================] - 1s 289us/step - loss: 0.1834 - accuracy: 0.8875\n",
      "Epoch 32/100\n",
      "2336/2799 [========================>.....] - ETA: 0s - loss: 0.1799 - accuracy: 0.8921"
     ]
    }
   ],
   "source": [
    "absa_model.fit(reviews_tokenized, dummy_category, epochs=100, verbose=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 2.5042 - accuracy: 0.3475\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 1.3553 - accuracy: 0.5735\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 1s 300us/step - loss: 0.6678 - accuracy: 0.7995\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 1s 294us/step - loss: 0.3473 - accuracy: 0.9100\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 0.2042 - accuracy: 0.9505\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 0.1282 - accuracy: 0.9680\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 0.0810 - accuracy: 0.9820\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 0.0552 - accuracy: 0.9890\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 0.0381 - accuracy: 0.9905\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 0.0236 - accuracy: 0.99400s - loss: 0.0295 \n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 1s 300us/step - loss: 0.0151 - accuracy: 0.9970\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 0.0082 - accuracy: 0.9985\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 1s 308us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 1s 316us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 1s 302us/step - loss: 6.6632e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 5.1989e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 4.2786e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 3.6127e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 3.1390e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 2.7517e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 1s 300us/step - loss: 2.4310e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 2.1301e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 1.9430e-04 - accuracy: 1.00000s - loss: 1.7077e-04 \n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 1.7210e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 1.5533e-04 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 1.4419e-04 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 1.2855e-04 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 1s 300us/step - loss: 1.1806e-04 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 1.0705e-04 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 9.9113e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 9.0925e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 1s 307us/step - loss: 8.4199e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 7.8349e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 1s 304us/step - loss: 7.2529e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 6.7094e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 6.2573e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 1s 305us/step - loss: 5.8756e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 1s 307us/step - loss: 5.4973e-05 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 5.1392e-05 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 1s 302us/step - loss: 4.7712e-05 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 4.4673e-05 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 4.2096e-05 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 3.9418e-05 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 3.6714e-05 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 3.4664e-05 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 3.2815e-05 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 1s 299us/step - loss: 3.0722e-05 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 2.9012e-05 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 1s 299us/step - loss: 2.7496e-05 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 2.6102e-05 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 2.4432e-05 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 2.3045e-05 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 1s 301us/step - loss: 2.1897e-05 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 2.0729e-05 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 1.9812e-05 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 1.8812e-05 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 1.7582e-05 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 1s 299us/step - loss: 1.6725e-05 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 1.5790e-05 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 1.5094e-05 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 1.4248e-05 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 1.3563e-05 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 1.2939e-05 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 1.2362e-05 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 1s 299us/step - loss: 1.1712e-05 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 1.1172e-05 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 1.0596e-05 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 1s 300us/step - loss: 1.0116e-05 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 1s 304us/step - loss: 9.5758e-06 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 1s 300us/step - loss: 9.1840e-06 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 1s 301us/step - loss: 8.7509e-06 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 1s 304us/step - loss: 8.2974e-06 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 7.9531e-06 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 7.6295e-06 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 7.2175e-06 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 1s 295us/step - loss: 6.9344e-06 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 6.5971e-06 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 1s 298us/step - loss: 6.3089e-06 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 6.0147e-06 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 1s 310us/step - loss: 5.7731e-06 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 1s 314us/step - loss: 5.5181e-06 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 1s 307us/step - loss: 5.3988e-06 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 1s 303us/step - loss: 4.9985e-06 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 1s 303us/step - loss: 4.7575e-06 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 1s 307us/step - loss: 4.5763e-06 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 1s 300us/step - loss: 4.3703e-06 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 1s 301us/step - loss: 4.2129e-06 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 1s 305us/step - loss: 4.0098e-06 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 1s 305us/step - loss: 3.8485e-06 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 1s 310us/step - loss: 3.7014e-06 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 1s 306us/step - loss: 3.4934e-06 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 1s 307us/step - loss: 3.3717e-06 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 1s 306us/step - loss: 3.2497e-06 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 1s 317us/step - loss: 3.0839e-06 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 1s 301us/step - loss: 2.9446e-06 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 1s 299us/step - loss: 2.8462e-06 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 1s 302us/step - loss: 2.7167e-06 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 1s 297us/step - loss: 2.6021e-06 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 2.4842e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd042dc1dd0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.fit(reviews_tokenized, dummy_sentiment, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = [\n",
    "    \"Good, fast service.\",\n",
    "    \"The hostess was very pleasant.\",\n",
    "    \"The bread was stale, the salad was overpriced and empty.\",\n",
    "    \"The food we ordered was excellent, although I wouldn't say the margaritas were anything to write home about.\",\n",
    "    \"This place has totally weird decor, stairs going up with mirrored walls - I am surprised how no one yet broke their head or fall off the stairs\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = [review.lower() for review in test_reviews]\n",
    "test_aspect_terms = []\n",
    "for review in nlp.pipe(test_reviews):\n",
    "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "    test_aspect_terms.append(' '.join(chunks))\n",
    "test_aspect_terms = pd.DataFrame(tokenizer.texts_to_matrix(test_aspect_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1 is expressing a   positive opinion about  SERVICE#GENERAL\n",
      "Review 2 is expressing a   positive opinion about  SERVICE#GENERAL\n",
      "Review 3 is expressing a   opinion about  FOOD#QUALITY FOOD#QUALITY\n",
      "Review 4 is expressing a   positive opinion about  FOOD#QUALITY\n",
      "Review 5 is expressing a   positive opinion about  AMBIENCE#GENERAL\n"
     ]
    }
   ],
   "source": [
    "test_sentiment_terms = []\n",
    "for review in nlp.pipe(test_reviews):\n",
    "        if review.is_parsed:\n",
    "            test_sentiment_terms.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]))\n",
    "        else:\n",
    "            test_sentiment_terms.append('') \n",
    "test_sentiment_terms = pd.DataFrame(tokenizer.texts_to_matrix(test_sentiment_terms))\n",
    "\n",
    "# Models output\n",
    "test_aspect_categories = label_encoder.inverse_transform(absa_model.predict_classes(test_aspect_terms))\n",
    "test_sentiment = label_encoder2.inverse_transform(sentiment_model.predict_classes(test_sentiment_terms))\n",
    "for i in range(5):\n",
    "    print(\"Review \" + str(i+1) + \" is expressing a  \" + test_sentiment[i] + \" opinion about \" + test_aspect_categories[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
